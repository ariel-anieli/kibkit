#!/usr/bin/python

import argparse
import datetime
import functools
import logging
import json
import operator
import random
import re
import requests
import string
import sys

logging.basicConfig(stream=sys.stdout, format='%(message)s', level=logging.INFO)

parser = argparse.ArgumentParser()
parser.add_argument('-u', '--user-credentials', required=True)
parser.add_argument('-H', '--host',             required=True)
parser.add_argument('-t', '--time-interval',    default='/PT1H')
group  = parser.add_mutually_exclusive_group(required=True)

group.add_argument('-ld',  '--list-dashboards', nargs='?', const=True)
group.add_argument('-lv',  '--list-visualizations')
group.add_argument('-sva', '--show-visualization-aggregations')
group.add_argument('-svd', '--show-visualization-data')
args = parser.parse_args()        

vz_and_db = (args.show_visualization_aggregations.split(',')[0],
            args.show_visualization_aggregations.split(',')[1], False) \
            if args.show_visualization_aggregations \
            else (args.show_visualization_data.split(',')[0],
                  args.show_visualization_data.split(',')[1], True) \
            if args.show_visualization_data else None

gen_rdm_hex = lambda i : random.choice(string.hexdigits)

base = 'http://{}@{}'.format(args.user_credentials, args.host)
head = {'Content-Type' : 'application/json',
        'kbn-xsrf'     : ''.join(map(gen_rdm_hex, range(32)))
}

def fold_if_true_and_apply(args, *funcs, _if=lambda pred: pred):
    return [functools.reduce(
        lambda _in, func: func(_in),
        funcs,
        arg) for arg in args if _if(arg)]

def build_time_interval(_time):

    def scan_time(init, pattern):
        match = re.search(pattern[0], init['sch']).group(0) \
            if re.search(pattern[0], init['sch']) else 0
        init['sch'] = re.sub(pattern[1], '', init['sch'])
        init['fnd'].append(int(match))

        return init

    return {
        key : value
        for key,value in zip(
                ['year',
                 'month',
                 'day',
                 'hour',
                 'minute',
                 'second']
                , functools.reduce(
                    scan_time,
                    [('(?<=P)\d+(?=Y)',  '(?<=P)\d+Y'),
                     ('(?<=P)\d+(?=M)',  '(?<=P)\d+M'),
                     ('(?<=P)\d+(?=D)',  '(?<=P)\d+D'),
                     ('(?<=PT)\d+(?=H)', '(?<=PT)\d+H'),
                     ('(?<=PT)\d+(?=M)', '(?<=PT)\d+M'),
                     ('(?<=PT)\d+(?=S)', '(?<=PT)\d+S')],
                    {'fnd' : [],
                     'sch' : _time}
                )['fnd']
        )}

def get_params(query):
    url   = {'ld' : '/api/saved_objects/_find',
             'lv' : '/api/saved_objects/_bulk_get'}[query]

    _type = {'ld' : 'dashboard',
             'lv' : 'dashboard'}[query]

    return {'url'  : url,
            'type' : _type}

def parse_each_aggs(init, entry):

    if entry['type']=='avg':
        init['aggs'][entry['id']]  = {
            'avg': {
                'field' : entry['params']['field']
            }
        }
    
    elif entry['type']=='cardinality':
        init['aggs'][entry['id']]  = {
            'cardinality': {
                'field' : entry['params']['field']
            }
        }

    elif entry['type']=='terms':
        init['vars'][entry['id']] = {
            'terms': {
                'field' : entry['params']['field'],
                'order' : {
                    '_count' : entry['params']['order']
                },
                'size'  : entry['params']['size']
            }
        }

    elif entry['type']=='range':
        init['aggs'][entry['id']] = {
            'range': {
                'field' : entry['params']['field'],
                'ranges' : entry['params']['ranges']
            }
        }

    return init

if __name__=="__main__":

    if args.list_dashboards:

        get_db = lambda params: requests.get(
            base + params['url'],
            headers = head,
            params  = {'type': params['type']})

        get_id_and_tle_of_db = lambda objs: fold_if_true_and_apply(
            objs,
            lambda obj: (obj["id"],obj["attributes"]["title"]),
            lambda out: "{},{}".format(out[0], out[1]))

        fold_if_true_and_apply(
            ['ld'],
            lambda _in  : get_db(get_params(_in)),
            lambda resp : json.loads(resp.text)["saved_objects"],
            lambda objs : '\n'.join(get_id_and_tle_of_db(objs)),
            logging.info)

    elif args.list_visualizations:

        get_db  = lambda ARGS : requests.post(
            base + ARGS['url'],
            headers = head,
            json    = [{'type': ARGS['type'],
                        'id'  : args.list_visualizations}])

        get_vz  = lambda _id: requests.post(
            base + get_params('lv')['url'],
            headers = head,
            json    = [{'type' : 'visualization',
                        'id'   : _id}])

        get_vz_tle_in_dashboard = lambda refs: fold_if_true_and_apply(
            refs,
            lambda ref  : get_vz(ref['id']),
            lambda req  : json.loads(req.text)['saved_objects'],
            lambda objs : objs[0]['attributes'],
            lambda atts : json.loads(atts['visState'])['title'],
            _if=lambda obj: obj['type']=='visualization')

        fold_if_true_and_apply(
            ['lv'],
            lambda _in  : get_db(get_params(_in)),
            lambda rsp  : json.loads(rsp.text)['saved_objects'],
            lambda objs : objs[0]['references'],
            lambda refs : '\n'.join(get_vz_tle_in_dashboard(refs)),
            logging.info)

    elif args.show_visualization_aggregations or args.show_visualization_data:
        vz, db, choice = vz_and_db

        if re.match('^\d.*P', args.time_interval):
            _date, span    = args.time_interval.split('/')
        elif not re.search('P', args.time_interval):
            _date = args.time_interval
            span = 'P1H'
        elif re.search('^P', args.time_interval):
            _date = ''
            span = args.time_interval

        add_or_sub = operator.add if _date else operator.sub

        get_time = lambda _date: datetime.datetime.utcnow() \
            if not _date else datetime.datetime.fromisoformat(_date)

        add_time_in_qry = lambda aggs, args: {
            'aggs'  : aggs['aggs'],
            'query' :
            {'bool' :
             {'filter' :
              [{'range' :
                {'timestamp' :
                 {'lte' : args['shift'] if args['op']==operator.add else args['start'],
                  'gte' : args['shift'] if args['op']==operator.sub else args['start']
                 }
                }
            }]
        }
            }
        }

        build_payload_from_aggs = lambda aggs: {
            'aggs' : {var_k : {
                'terms' : aggs['vars'][var_k]['terms'],
                'aggs'  : aggs['aggs']
            } for var_k in aggs['vars']} if aggs['vars'] else aggs['aggs']}

        get_db      = lambda ARGS: requests.post(
            base + ARGS['url'],
            headers = head,
            json    = [{'type': ARGS['type'], 'id': db}])

        get_vz      = lambda _id: requests.post(
            base + get_params('lv')['url'],
            headers = head,
            json    = [{'type': 'visualization', 'id': _id}])

        get_vz_data = lambda QRY: requests.post(
            base + '/elasticsearch/dev_*/_search?',
            json    = QRY,
            headers = head,
            params  = {'timeout'                : '30000ms',
                       'rest_total_hits_as_int' : 'true',
                       'ignore_unavailable'     : 'true',
                       'ignore_throttled'       : 'true'})

        get_all_vz_in_db    = lambda rsp: fold_if_true_and_apply(
            json.loads(rsp.text)['saved_objects'][0]['references'],
            lambda obj   : get_vz(obj['id']),
            lambda rsp   : json.loads(rsp.text)['saved_objects'],
            lambda objs  : objs[0]['attributes'],
            lambda atts  : json.loads(atts['visState']),
            _if=lambda obj: obj['type']=='visualization')

        shift_time = lambda _date, shift, add_or_sub: {
            'op'    : add_or_sub,
            'start' : _date.isoformat(),
            'shift' : _date.replace(
                year   = add_or_sub(_date.year,   shift['year']),
                month  = add_or_sub(_date.month,  shift['month']),
                day    = add_or_sub(_date.day,    shift['day']),
                hour   = add_or_sub(_date.hour,   shift['hour']),
                minute = add_or_sub(_date.minute, shift['minute']),
                second = add_or_sub(_date.second, shift['second'])
            ).isoformat()
        }

        if_true_get_vz_data = lambda choice, vz: functools.reduce(
            lambda _in, func: func(_in),
            [parse_all_aggs,
             build_payload_from_aggs,
             lambda load: (load, build_time_interval(span)),
             lambda _: (_[0], shift_time(get_time(_date), _[1], add_or_sub)),
             lambda _: add_time_in_qry(_[0], _[1]),
             get_vz_data,
             lambda rsp: json.loads(rsp.text)['aggregations']
            ], vz) if choice else vz

        parse_all_aggs   = lambda vz: functools.reduce(
            parse_each_aggs, vz, {'vars':{}, 'aggs':{}})

        select_vz = lambda _all: [_vz['aggs']
                                  for _vz in _all
                                  if _vz['title']==vz].pop()

        unpack_query = lambda rsp: json.loads(
            functools.reduce(lambda dct, key: dct[key],
                             ['saved_objects',
                              0,
                              'attributes',
                              'kibanaSavedObjectMeta',
                              'searchSourceJSON'],
                             json.loads(rsp.text)))['filter'][0]['query']

        fold_if_true_and_apply(
            ['lv'],
            lambda param : get_db(get_params(param)),
            get_all_vz_in_db,
            select_vz,
            lambda vz: if_true_get_vz_data(choice, vz),
            json.dumps,
            logging.info)
